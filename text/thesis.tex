%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  color, %% This option enables colorful typesetting. Replace with
         %% `monochrome`, if you are going to print the thesis on
         %% a monochromatic printer.
  table, %% Causes the coloring of tables. Replace with `notable`
         %% to restore plain tables.
  lof,   %% Prints the List of Figures. Replace with `nolof` to
         %% hide the List of Figures.
  lot,   %% Prints the List of Tables. Replace with `nolot` to
         %% hide the List of Tables.
  %% More options are listed in the class documentation at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/fithesis/guide/mu/fi.pdf>.
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.

\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
%%\usepackage{paratype}
%%\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
\usepackage{xcolor} 
\newcommand{\todo}[1]{\textcolor{red}{\textbf{#1}}}
\usepackage{listings}
\usepackage[binary-units=true]{siunitx}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    university    = mu,
    faculty       = fi,
    type          = mgr,
    author        = Samuel Petroviƒç,
    gender        = m,
    advisor       = Adam Rambousek,
    title         = {Performance testing of Virtual Data Optimizer storage layer},
    TeXtitle      = {Performance testing of Virtual Data Optimizer storage layer},
    keywords      = {file system, performance, aging, benchmarking, fragmentation, storage, trim, fs-drift, XFS, VDO},
    TeXkeywords   = {file system, performance, aging, benchmarking, fragmentation, storage, trim, fs-drift, XFS, VDO},
assignment = {}
}
\thesislong{abstract}{
Abstrakt sa pise nakoniec
}
\thesislong{thanks}{
I would like to thank my self for tremendous help and guidance during writing of this thesis. I would also like to thank Red Hat for collaboration and provision of necessary testing equipment. 
}
%% The following section sets up the bibliography.


\usepackage{csquotes}
\usepackage[              %% When typesetting the bibliography, the
  backend=bibtex,          %% `numeric` style will be used for the
  style=numeric,          %% entries and the `numeric-comp` style
  citestyle=numeric-comp, %% for the references to the entries. The
  sorting=none,           %% entries will be sorted in cite order.
  sortlocale=auto         %% For more unformation about the available
]{biblatex}               %% `style`s and `citestyles`, see:
%% <http://mirrors.ctan.org/macros/latex/contrib/biblatex/doc/biblatex.pdf>.
\addbibresource{citations2.bib} %% The bibliograpic database within

                          %% the file `example.bib` will be used.
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{menukeys}
\usepackage{pdfpages}

\renewcommand{\lstlistingname}{Example}
\renewcommand{\lstlistlistingname}{List of \lstlistingname s}



\begin{document}
\chapter{Introduction}
Storage requirements of modern society grows exponentialy, but even while cost of storage devices is decreasing, new solutions for effective data storing need to be implemented. In Linux, large family of storage administration tools called Logical Volume Manager (i.e. LVM) is available to users.

By using LVM tools and other tools available in kernel, users are able to create complex logical structures above physical devices. This complex structure of physical device management is generally called storage stack. In storage stack, different layers represent needed form of abstraction for working with the devices. These layers can have multitude of functions such as software RAID, encrypting, caching, back up, thin provisioning or as of now compression.

Compression is delivered by installing the Virtual Data Optimizer (or VDO) layer. This layer is able to deduplicate and compress user data on the fly, making it possible to create logical volumes larger than available physical space.

This approach seems very advantageous to users, since the cost of installing such layer is much smaller than the cost of buying more physical storage. However, obvious questions arise in terms of performance impact.

In this thesis, such questions will be addresed by thorogh performance testing of VDO layer as well as covering VDO functionality, usage and administration.

This thesis is a follow up on the tesis The Effects of Age on File System Performance and will use similar approaches and testing methods.

In Chapter 3, VDO will be introduced. The reader will get valuable information on VDO terminology, compression and deduplication techniques, device organisation, system requirements, administration, positioning with other layers etc.

In Chapter 4, new features in the benchmark fs-drift will be introduced. The fs-drift benchmark had to undergo some changes so it can be able to test compression layer better. Some changes to data measurements and reporting will also be introduced.

In Chapter 5, proposed testing techniques and configurations will be explained.

In Chapter 6, results and performance impact of administering VDO layeer will be discussed.





\chapter{Related work}
\label{related}
In this chapter I present different approaches of file system aging and fragmentation research described and implemented in the past. The first section discuss usage of collected data to create aging workload. The second section discuss possibilities of aging the file system artificially, without pre-collected data.




\chapter{Virtual Data Optimiser}
\label{VDO}
\section{Introduction}
Virtual Data Optimizer (VDO) is a compression layer in kernel storage stack. Using block virtualisation, it allows for users to operate with a logic volume much greater than physically available. This sort of overprovision is achieved by using block deduplication an block compression.

Deduplication is a technique that, on a block level, dissalows multiple copies of the same data to be written on physical device. In VDO, duplicate blocks are detected but only one copy is physically written. Subsequent copies then only reference the address of that written block. These blocks are therefore called shared blocks.

Compression is a technique that reduces usage of physical device by identifying and eliminating redundancy in data. In VDO, lossless HIOPS compression, based on a parallelized packaging algorithm is used to handle many compression operations at once. Compressed blocks are stored in a way that allows the most logical blocks to be stored in one physical block.

The actual VDO technology consists of two kernel modules. First module, called \texttt{kvdo}, loads into the Linux Device Mapper layer to provide a deduplicated, compressed, and thinly provisioned block storage volume. Second module called \texttt{uds} communicates with the Universal Deduplication Service (UDS) index on the volume and analyzes data for duplicates.

\subsection{Deduplication}
Deduplication in VDO relies on growing UDS index.

Any incomming block requested to be written is hashed using mur-mur hash. Mur-mur hash is not completely collisionless, but it's a trade of for speed, that is much needed for on-the-fly deduplication.

The resulting hash is then searched for in the UDS index. In case the index contains an entry with the same hash, $kvdo$ reads the block from physical device and compare it to the requested block byte-by-byte to ensure they are actually identical.

In case they are, block map is upadted in a way that the logical block points to the physical block that's already written. If the index doens't contain the computed hash or the block-by-block comparison indetifies a difference in the blocks, $kvdo$ updates the block map to make and stores the requested block.

Either way, the block's hash is written to the beginning of the index. This index is held in memory to present quick deduplication advice to the VDO volume, therefore there is a minimum requirement of $\SI{250}{\mega\byte}$ of DRAM to be available.


\subsection{Compression}
Compression  is an important part of VDO data processing. By compressig already deduplicated blocks, the volume saves even more space. Compression phase is also importatnt for saving space in case the user data don't deduplicate well.

In VDO, lossless HIOPS compression, based on a parallelized packaging algorithm is used to handle many compression operations at once. Compressed blocks are stored in a way that allows the most logical blocks to be stored in one physical block.

\subsection{Zero-block elimination}
Zero-blocks are blocks that are composed completely of zeroes. These blocks are handled differently that normal and duplicate data blocks.

If the VDO layer detects a zero-block, it will treat it as a discard, thus VDO will not send a write request to the undelying layer. Instead, VDO will mark the block as free on a physical device (if it was not a share block) and updates it's block map.

Because of this, if user wants to manually free some space, they can store a file filled with binary zeroes and deleting it.


\section{VDO Layer}
VDO layer is actually another block device that can aggregate physical storage, partitions etc. On creation of VDO volume, management tool also creates volume for UDS index as well as for the actual VDO. 

\subsection{Physical Size}
The VDO volume is divided into continuous regions of physical space of constant size. These regions are called slabs and maybe of size of any power of 2 multiple of $\SI{128}{\mega\byte}$  up to $\SI{32}{\giga\byte}$ . After creating VDO volume, the slab size cannont be changed. However, a single VDO volume contain only up to 8096 slabs, so the configured size of slab at VDO volume creation determines its maximum allowed physical storage. Since the maximum slab size is $\SI{32}{\giga\byte}$ and maximum number of slabs is 8096, the maximum volume of physical storage usable by VDO is therefore $\SI{256}{\tera\byte}$. Important thing to notice is that at least one slab will be reserved for VDO metadata and wouldn't be used for storing data. Slab size does not affect VDO performance.

When trying to examine physical size, two terms are offered. The term Physical size stands for the overall size of undelying device. Available physical size stands for the portion of physical size, that can actually hold user data. The part that does not hold user data is used for storing VDO metadata, f.e. UDS index.

\subsection{Logical Size}
The concept of VDO offers the user means for overprovisioning the physical volume. During creation of VDO volume, user can specify logical size of volume, which can be much larger than the size of physical underlying storage. The user should be able to predict the compressibility of future incoming data and set the logical volume accordingly. At maximum, VDO suports up to 254 times the size of physical volume which amounts to maximum logical size of $\SI{4}{\peta\byte}$.

\subsection{Memory requirements}
The VDO module itself requires $\SI{370}{\mega\byte}$ and additional $\SI{268}{\mega\byte}$ per every $\SI{1}{\tera\byte}$ of used physical storage. Users are therefore expected to compute the needed memory volume and act accordingly.

Another module that consumes memory is the UDS index. However, several mechanisms are in place to ensure the memory consumption does not offset the advantages of VDO usage.

There are two parts to UDS memory usage. First is a compact representation in RAM that contains at most one entry per unique block, that is used for deduplication advice. Second is stored on-disk that keeps track of all blocks presented to UDS. The part stored in RAM tracks only most recent blocks and is called $deduplication window$. Despite it being only index of recent data, most datasets with large levels of possible deduplication also show a high degree of temporal locality, according to developers. This allows for having only a fraction of the UDS index in memory, while still mantaining high levels of deduplication. Were not for this fact, memory requirements for UDS index would be so high that it would out-cost the advantages of VDO usage completely.

For better memory usage, UDS's Sparse Indexing feature was introduced to the uds module. This feature further exploits the temporal locality quality by holding only the most revelant index entries in the memory. Using this feature (which is reccomended default for VDO) allows for maintaining up to ten times larger deduplication window whlie maintaining the same memory requirements.

\subsection{VDO kernel module}
The $kvdo$ module provides mentioned techniques within Linux device mapper level. Device mapper serves as a framevork for storage and block device management. The kvdo module presents itself as a block device that can be accessed directly as a raw device or via installation of supported file systems (XFS/EXT4). 

After receiving read request from an above structure, $kvdo$ maps the requested logical address to the actual physical block and retrieves the data.

When $kvdo$ receives a write reqest, it updates its block map and acknowledges the request. If the received request is either DISCARD, TRIP or a block of only zeroes, $kvdo$ only acllocates a physical block for the request.

\subsection{VDO write policies}
VDO can operate in either synchronous or asynchronous mode. By default VDO write policy is set to $auto$ which means the the module decides automatically which write policy to enquire. The main difference is in the approach if the block is written immediately or not. The obvious consequence is that if a system fails while VDO performs asynchronous write, user can lose data.

In synchronous mode, the block is temporarily written to an allocated block and acknowledges the request. After completing the acknowledgement, it attempts to deduplciate the block. In case it's a duplicate, block map is updated in a way that the logical block points to the physical block that's already written and releases the temporal block. If the block is not a duplicate, $kvdo$ updates the block map to make the temporarily allocated physical block permanent.

In asynchronous mode, instead of writing the data immediately, only physical block allocation and acknowledgement of the request is performed. Next, VDO will attempt to deduplicate the block. If the block is a duplicate, the module only updates it's block map and releases the allocated block. If the block turns out to be unique, it is then written to the allocated block and the block map is updated.

\subsection{Storage requirements}
As mentioned earlier, at least one $slab$ is reserved for VDO metadata and UDS on-disk index.

VDO module keeps two kinds of metadata which differ in the scale of required space.
\begin{compactenum}
\item type scales with physical size and uses about $\SI{1}{\mega\byte}$ per every $\SI{4}{\giga\byte}$ of managed physical storage and also additional $\SI{1}{\mega\byte}$ per $slab$. 
\item type scales with logical size and uses approximately $\SI{1.25}{\mega\byte}$ for every $\SI{1}{\giga\byte}$ of logical storage, rounded up to the nearest slab.
\end{compactenum}

\subsection{VDO in Storage Stack}
\label{stack}
Generally it is importatnt for users to realise that some of the storage layers work better when above or under the VDO layer in the storage stack.

Technology that is recommended to be installed under VDO layer:

\begin{itemize}
  \item dm-multipath
  \item dm-crypt, i.e. layer for data encryption
  \item mdraid, i.e. software raid
  \item LVM (as software raid)
\end{itemize}

Technology that is recommended to be installed above VDO layer:

\begin{itemize}
  \item LVM cache, i.e. possibility to mark part of a block device as a cache to be used by LVM
  \item LVM Snapshots
  \item LVM Thin Provisioning
\end{itemize}

Unsupported configurations are the ones that break those rules plus a few others:

\begin{itemize}
  \item VDO on top of VDO volume
  \item VDO on top of LVM Snapshots
  \item VDO on top of LVM Cache
  \item VDO on top of LVM Thin pool
  \item VDO on top of a loopback device
  \item VDO under an encrypted device
  \item Partitions on VDO volume
  \item RAIDs on top of VDO volume          
\end{itemize}

\section{Administering VDO}
\subsection{Installation}
Since VDO is now part of Kernel, it can be installed usign native packaging system. VDO relies on two RPM packages to be installed:
\begin{itemize}
    \item vdo
    \item kmod-kvdo
\end{itemize}
After succesfull installation of these two packages, user can create a VDO volume.

\subsection{Creating VDO volume}
VDO can be created using VDO manager through command line by invoking $vdo create$.
The most important parameters are:
\begin{itemize}
    \item --name=vdoname
    \item --device=blockdevice
    \item --vdoLogicalSize=logicalsize
    \item --vdoSlabSize=slabsize
\end{itemize}

When specifying block device, it is reccommended to use persistent device name. Otherwise, VDO might fail to start properly in case the name of the device changes.

While specifying the logical space, user should be aware what kind of data will be written into the VDO block device and set the logical size accordingly. If heavily compressible data are expected, user can specify logical size as large as ten times the physical size. If the data are expected to be less compressible, it is reccomended to lower the ratio accordingly.

After succesfull VDO creation, the layer is prepared to be used as an ordinary block device. That means, either file system can be created on top of it, or a more complex structure can be installed above. All within the contstrains specified in section~\ref{stack}.

\subsection{Monitoring VDO}
VDO works as a thinly provisioned volume. Therefore applications or file systems that use it will only see a logical space that is provided by VDO. To monitor VDO space usage, physical space left or compression ratio and much more, $vdostats$ utility provides neccessary inspection of VDO volume.

\chapter{Fs-drift}
Fs-drift is a benchmark developed specifically for testing file system aging performance. It is currently developed as an open source project by engineer Ben England and Samuel Petroviƒç. During the years of collaboration, both sides usually add features that are important for the individuals line of work, therefore there are currently two working branches. One for testing of shared file systems as Ceph and the like and second for testing local layers that require more advanced testig as VDO or Thin Provisioning testing. 

\section{New features}
Fs-drift was used as a main tool in a previous study of file system aging so only new features will be presented in this thesis.

\subsection{Compressible data generator}
For testing a compression layer as VDO, completely random data cannot be used. The benchmark needs to be able to shape the incomming data in a way that simulates compressibility that is expected in real-life user data. For this reason, new parameter was added to fs-drift, which specifies what compress ratio will the incomming data have.

The parameter in question works as follows:
\begin{itemize}
    \item -c|--compression-ratio, number that is a desired compress ratio, e.g. 4.0 is 1/4.0, therefore compressibility will be 75\%~(default~0.0)
\end{itemize}

This feature was implemented using alternative IO buffer behavior which works with another open source project called $lzdatagen$.

LZ data generator (i.e. lzdatagen) is a simple but powerfull tool for generating data with desired compressibility. In the simples use cases, user only specifies the desired size and compress ration and aquires data with the exact qualities.

Usage is:
\begin{itemize}
    \item ./lzdatagen --size SIZE --ratio RATIO
\end{itemize}

The new approach in the buffer therefore retreives the desired compression ratio, if it's set on 0.0, normal data are generated as in previous versions. If there is a compression ratio set, the buffer is filled using lzdatagen call with the desired compression ratio.

To address concerns of lzdatagen affecting performance measurements, the generator is filled with data before the measurement is started.

This simple but powerfull feature now empowers the benchmark user to specify the compressibility of written data and therefore makes the user able to measure performance of any layer with a compression feature such as ZFS or VDO.

\subsection{Data reporting}
In previous versions, several problems in measurement reporting had to be addressed.

One of the problems was the data storage. In older versions, data were stored in-memory which posed two problems when running very long tests:
\begin{itemize}
    \item RAM consumption
    \item Lose of data after benchmark failure
\end{itemize}

The new approach that was installed uses a file predestined on a stable device (i.e. system device). Every time the measurement is made, an entry is appended to the file. This means that even after many days of testing, the memory of a system si not cluttered. On top of that, if the system or benchmark fails, there is still some data to be retreived from the file.

Second problem that caused noisy performance data in the previous versions was the use of units unsuitable for local performance testing. The unit in question was response time which may be important when measuring performance in distributed file systems but in the kind of testing the previous work and this work is trying to achieve, measuring throughtput would yield much more stable data points.

Therefore, the option to store bandwidth was added as a new feature. Bandwidth is measuerd as a size of request divided by the time of request completion. This way, variability of request size will not be affecting the data points as we could see while using response times where usually larger IO requests will take more time to complete.


\subsection{Performance measurement}
In fs-drift, performance is measured by saving a time stamp before invoking the IO operation and storing the time stamp difference after the operation was finished. However, with this type of measurment, it is important to make sure the time stamping is as close to the actual IO operations as possible. 

In previous versions, the time measurement was the same for every IO operation, which made some of the measurement incosistent, f.e. taking into the measurement the time to fill buffer with lzdatagen.

This problem was removed in the latest versions. Changes to the code were made in a way that for every IO operation, concious desision can be made of where to begin and end the measurement. This feature makes performance readings much more stable and reliable.


\chapter{Testing hardware}
In this chapter, I will introduce testing systems which will be used for multitude of tests with or without the VDO layer installed. These machines were chosen by their computing power, provided memory and by useful storage hardware they are equiped with. These machines are stable systems used by Red Hat Kernel Performance team for regular testing.

\label{machines}
\section{Supermicro X11SPL-F}
This machine is used for regular testing of VDO and other complex technologies in Red Hat Kernel Performance team. It's equiped with 4 $\SI{10}{\tera\byte}$ SAS rotational drives and one $\SI{220}{\giga\byte}$ SATA SSD for tests that require LVM Cache. The system is always installed on an additional SSD. This machine is equiped with enough memory to handle large VDO volumes.
\clearpage
\begin{tabular}{|l|l|}
\hline
   \multicolumn{2}{|l|}{Machine\,1} \\ \hline %draven
    Model & Supermicro X11SPL-F\\
    \hline
    Processor & Intel Xeon Silver 4110  \\
    \hline
    Clock speed & $\SI{2.10}{\giga\hertz}$ (8 cores) \\
    \hline
    Memory & $\SI{49152}{\mega\byte}$ \\

\hline
   \multicolumn{2}{|l|}{Testing Hard Drives (4x)} \\ \hline %draven
    Model & WD HGST Ultrastar\\
    \hline
    Capacity & $\SI{1}{\tera\byte}$  \\
    \hline
    Interface & SAS $\SI{12}{\giga\byte}$  \\
    \hline
    Type & Rotational HDD \\
    \hline    
    Logical sector size & $\SI{4096}{\byte}$ \\    
    \hline    
    Physical sector size & $\SI{4096}{\byte}$ \\
    \hline
    
   \multicolumn{2}{|l|}{Testiting SSD (for LVM cache)} \\ \hline %draven
    Model & Micron 5100 MTFD \\
    \hline
    Capacity & $\SI{240}{\giga\byte}$  \\
    \hline
    Interface & SATA $\SI{6}{\giga\byte}$  \\
    \hline
    Type & SSD \\
    \hline    
    Logical sector size & $\SI{512}{\byte}$ \\    
    \hline    
    Physical sector size & $\SI{4096}{\byte}$ \\
    \hline
    
    \multicolumn{2}{|l|}{System disk} \\ \hline %draven
    Model & SuperMicro SSD  \\
    \hline
    Capacity & $\SI{126}{\giga\byte}$  \\
    \hline
    Interface & PCIe Gen3 x4 Lanes  \\
    \hline
    Type & SSD \\
    \hline    
    Logical sector size & $\SI{512}{\byte}$ \\    
    \hline    
    Physical sector size & $\SI{512}{\byte}$ \\
    \hline   
\end{tabular}






\section{Machine2}

\chapter{Tested configurations}
\label{storageConf}
In testing of specific storage stack layer, it is important to have performance assesment of a stack without that layer installed. These configurations are called baseline configurations. In this thesis, baseline configurations will used striped LVM to join hard drives as well as LVM cache to simulate real-life usage.

After the baseline testing is finished, tests with VDO layer installed will be run. For non-cached version, only simple VDO layer will be installed with file system on top of it. In the cached version, VDO layer will be installed under the LVM-cache.


\section{Baseline configurations}
Baseline configurations will be storage stacks without the VDO layer installed. Their purpose is to provide a stable testing enviroment to determine performance of undelying hardware and performance of other layers in storage stack.

\subsection{Striped LV}
This baseline configuration will consist of simple stack built with LVM on top of two rotational hard drives. The striping of 2 will be used to balance the load on both devices equally. The commands to create this simple stack is shown in example~\ref{ex:simpleLV}

\lstset{language=bash, 
numbers=none, 
frame=single, 
commentstyle=\color{dkgreen}, 
basicstyle={\scriptsize\ttfamily}, 
keywordstyle=\color{blue}, 
%identifierstyle=\color{blue}, 
stringstyle=\color{red},
captionpos=t,
showstringspaces=false,
breaklines=true,
breakatwhitespace=false,
tabsize=3,
caption={sdad},
}


\begin{lstlisting}[language=bash, label={ex:simpleLV}, caption={Creating striped logical volume}][frame=single]
    $ pvcreate /dev/sd[bc] -y
    $ vgcreate vg /dev/sd[bc] -y
    $ lvcreate -n testLV -i2 -l100%vg vg /dev/sd[bc] -y
    $ mkfs.xfs /dev/mapper/vg-testLV        
\end{lstlisting}

\subsection{Cached LV}
This baseline configuration will consist of a striped LV over two rotational hard drives supported with an LVM Cache hosted on a separate solid state device. These devices must belong to the same Volume Group. Creating LVM Cache consist of several steps. First a small part of the SSD needs to be reserved for LVM Cache metadata. This can be done by creating small LV on the device and converting that LV to cache-pool type. Next, we need to mark the portion of SSD we want to use for caching by creating another LV. We'll create another LV to join the two hard drives with striping of 2. At the end, we convert the storage LV to a cached volume. The commands to create this cached volume is shown in example~\ref{ex:cachedLV}

\begin{lstlisting}[language=bash, label={ex:cachedLV}, caption={Creating cached logical volume}][frame=single]
    $ pvcreate /dev/sd[bce] -y
    $ vgcreate vg /dev/sd[bce] -y
    $ lvcreate -n cacheMeta -L 44M vg /dev/sde -y
    $ lvcreate -n cache -L 200G vg /dev/sde -y
    $ lvcreate -n testLV -i2 -l100%vg vg /dev/sd[bc]  -y
    $ lvconvert --type cache-pool --poolmetadata vg/cacheMeta vg/cache -y
    $ lvconvert --type cache --cachepool vg/cache vg/testLV -y
    $ mkfs.xfs /dev/mapper/vg-testLV        
\end{lstlisting}


\section{Configurations with VDO}
After determining the performance without VDO layer installed, we can continue with actual performance testing of VDO. Important thing to notice is that when creating a file system on top of VDO volume, the file system will attempt to discard the whole volume to ensure a clean begining state. While testing this isn't really an important feature and with volumes of tens of tera bytes, it would take quite a long time to discard the whole VDO volume. Therefore mkfs options (-K for XFS) can be used to prevent it.

\subsection{VDO over striped LV}
This simple configuration will consist of VDO layer on top of two rotational hard drives joined by LV. The striping of 2 will be used to balance the load on both devices equally. The commands to create this stack is shown in example~\ref{ex:VDOLVC}

\begin{lstlisting}[language=bash, label={ex:VDOLV}, caption={Creating VDO over striped LV}][frame=single]
    $ pvcreate /dev/sd[bc] -y
    $ vgcreate vg /dev/sd[bc] -y
    $ lvcreate -n testLV -i2 -l100%vg vg /dev/sd[bc] -y
    $ vdo create --name=testVDO --device=/dev/mapper/vg-testLV --vdoLogicalSize=60T  --vdoSlabSize=8g --force
    $ mkfs.xfs -K /dev/mapper/vg-testLV      
\end{lstlisting}

\subsection{VDO under LVM Cache}
This configuration will consist of a VDO layer built on top of two rotational hard drives joined by LV. The striping of 2 will be used to balance the load on both devices equally. Sometimes the LVM can have a problem with adding devices with different block size to a single volume group. This can be avoided by allowign mixed block sizes in lvm configuration file. Also, when more complexity needs to be build above the VDO layer, it is important to create a physical volume on a VDO block device. The commands to create this stack is shown in example~\ref{ex:VDOLVCache}

\begin{lstlisting}[language=bash, label={ex:VDOLVCache}, caption={Creating cached VDO volume}][frame=single]
    $ pvcreate /dev/sd[bce] -y
    $ vgcreate vg /dev/sd[bc] -y
    $ lvcreate -n testLV -i2 -l100%vg vg /dev/sd[bc]  -y    
    $ vdo create --name=testVDO --device=/dev/mapper/vg-testLV --vdoLogicalSize=60T --vdoSlabSize=8g --force    
    $ sed -i '/devices {/a allow_mixed_block_sizes = 1' /etc/lvm/lvm.conf
    $ pvcreate /dev/mapper/testVDO -y
    $ vgcreate VG2 /dev/mapper/testVDO /dev/sde -y -ff
    $ lvcreate -n cacheMeta -L 44M VG2 /dev/sde -y
    $ lvcreate -n cache -L 200G VG2 /dev/sde
    $ lvcreate -n testLV2 -l100%vg VG2 /dev/mapper/testVDO
    $ lvconvert --type cache-pool --poolmetadata VG2/cacheMeta VG2/cache -y
    $ lvconvert --type cache --cachepool VG2/cache VG2/testLV2 -y
    $ mkfs.xfs -K /dev/mapper/VG2-testLV2
\end{lstlisting}


\chapter{Conducted tests}
For thorough testing of VDO, several types of tests needs to be executed. First section will aim to show differences in performance between loading VDO with heavily compressible data and non-compressible data. Next section will try to show differences in performance of VDO layer when under single threaded load and heavy multithreaded load. These tests will be held on all storage configurations from Chapter~\ref{storageConf}.

\section{Different compressibility}
The VDO layer can be used in multiple ways, when users want to save physical space. However, the level of compressibility of user data can vary. The aim of this test is to determine, whether compressibility of user data have impact on VDO performance or to what extent.

\section{Single vs. Multithread}
Since VDO usage is meant for simple users as well as for complex cases, it is important to know the differences between single-user load and heavy multithreaded traffic.





































\end{document}
