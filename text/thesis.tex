%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  color, %% This option enables colorful typesetting. Replace with
         %% `monochrome`, if you are going to print the thesis on
         %% a monochromatic printer.
  table, %% Causes the coloring of tables. Replace with `notable`
         %% to restore plain tables.
  lof,   %% Prints the List of Figures. Replace with `nolof` to
         %% hide the List of Figures.
  lot,   %% Prints the List of Tables. Replace with `nolot` to
         %% hide the List of Tables.
  %% More options are listed in the class documentation at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/fithesis/guide/mu/fi.pdf>.
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
%%\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.

\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
                %% german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
%%\usepackage{paratype}
%%\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
\usepackage{xcolor} 
\newcommand{\todo}[1]{\textcolor{red}{\textbf{#1}}}
\usepackage{listings}
\usepackage[binary-units=true]{siunitx}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    university    = mu,
    faculty       = fi,
    type          = mgr,
    author        = Samuel Petroviƒç,
    gender        = m,
    advisor       = Adam Rambousek,
    title         = {Performance testing of Virtual Data Optimizer storage layer},
    TeXtitle      = {Performance testing of Virtual Data Optimizer storage layer},
    keywords      = {VDO, deduplication, compression, storage, fs-drift},
    TeXkeywords   = {VDO, deduplication, compression, storage, fs-drift},
assignment = {}
}
\thesislong{abstract}{
Abstrakt sa pise nakoniec
}
\thesislong{thanks}{
THX
}
%% The following section sets up the bibliography.


\usepackage{csquotes}
\usepackage[              %% When typesetting the bibliography, the
  backend=bibtex,          %% `numeric` style will be used for the
  style=numeric,          %% entries and the `numeric-comp` style
  citestyle=numeric-comp, %% for the references to the entries. The
  sorting=none,           %% entries will be sorted in cite order.
  sortlocale=auto         %% For more unformation about the available
]{biblatex}               %% `style`s and `citestyles`, see:
%% <http://mirrors.ctan.org/macros/latex/contrib/biblatex/doc/biblatex.pdf>.
\addbibresource{citations2.bib} %% The bibliograpic database within

                          %% the file `example.bib` will be used.
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{menukeys}
\usepackage{pdfpages}

\renewcommand{\lstlistingname}{Example}
\renewcommand{\lstlistlistingname}{List of \lstlistingname s}



\begin{document}
\chapter{Introduction}
Storage requirements of modern IT industry grows exponentialy, but even while cost of storage devices is decreasing, new solutions for optimal storage utilisation need to be implemented. In Linux there is a large family of storage administration tools called \emph{Logical Volume Manager} (i.e. LVM) that provides users with means of managing storage.

By using LVM and other tools available in Linux Kernel, users are able to create complex, layered abstraction which helps them utilise physical storage in the way that suits their needs. This complex structure of physical device management is generally called storage stack. In storage stack, different layers represent needed form of abstraction for working with the devices. These layers can have multitude of functions such as software RAID, encryption, caching, back up, thin provisioning or space saving.

Space saving is delivered by installing the \emph{Virtual Data Optimizer} (i.e. VDO) layer. This layer is able to deduplicate and compress user data on the fly, making it possible to create logical volumes larger than available physical space. This storage is therefore \emph{thinly provisioned}, however is different from the thin pool technology, because (given the user data are compressible enough), it can actually hold more data.

There are two main techniques used by VDO to save space. The first one is \emph{deduplication}, which is a way to identify and eliminate multiple copies of the same data, preventing them to occupy physical space. The second technique is \emph{compression}, that shrinks already deduplicated data furthering the space saving ratio.

Space saving presents users with serious advantage, since the cost of installing and maintaining a layer like this is much lower than the cost of purchasing more physical storage. However, on the fly manipulations of large quantities of data may seem costly in terms of usage of other resources like memory or CPU.

An important part of VDO technology is to make sure the costs of running VDO does not out-cost other approaches like purchasing more resources. VDO includes multitude of internal structures and logic, tunable by users, to ensure its advantages.

Because of aformentioned reasons, performance testing is an important part of developing and administering VDO. Users and developement teams need to ensure the final product meets industrial quality standards.

However, performance testing of such a complex technlogy requires extended knowledge of its internal structures and advanced expertise in benchmarking. 

This thesis aims to lay a foundational work for performance testing of VDO, describing its workings, performance related structures and issues and describing ways of benchmarking VDO. 

Chapter~\ref{VDO} is an introduction of VDO technology. Space saving mechnaisms will be described as well as VDO terminology, device organisation, system requirements, administration and relation to other layers in storage stack.

Chapter~\ref{benchmarks} presents used benchmarks, FIO and fs-drift, and their usage in testing VDO technology. New features for testing VDO were implemented for fs-drift and will be described in this chapter.

Chapter~\ref{testing} is focused on performance testing of VDO. The testing is aimed on different VDO components as well as more complex deployment cases. Either way, results of the testing are presented giving recommendation for VDO usage and testin its performance.

In Chapter~\ref{conclusion}, high-level insight on performance testing of VDO is given with recommendation for further work.


\chapter{Virtual Data Optimiser}
\label{VDO}
\section{Introduction}
Virtual Data Optimizer (VDO) is a block layer virtualisation service in Linux storage stack. VDO enables user to operate with greater logical volume than it's physically available. This is achieved by using compression and deduplication.

Deduplication is a technique that, on a block level, dissalows multiple copies of the same data to be written to physical device. In VDO, duplicate blocks are detected but only one copy is physically stored. Subsequent copies then only reference the address of the stored block. Blocks that are deduplicated therefore share one physical address.

Compression is a technique that reduces usage of physical device by identifying and eliminating redundancy in data. In VDO, lossless compression, based on a parallelized packaging algorithm is used to handle many compression operations at once. Compressed blocks are stored in a way that allows the most logical blocks to be stored in one physical block.

The actual VDO technology consists of two kernel modules. First module, called \texttt{kvdo}, loads into the Linux Device Mapper layer to provide a deduplicated, compressed, and thinly provisioned block storage volume. Second module called \texttt{uds} communicates with the Universal Deduplication Service (UDS) index on the volume and analyzes data for duplicates.

\subsection{Deduplication}
Deduplicaton limits writing multiple copies of the same data by detecting duplicate blocks. Blocks that are duplicate of a block that VDO has already seen are stored as references for that block, which saves space on the underlying device.

Deduplication in VDO relies on growing UDS index. Hash from any incomming block, requested to be written, is searched for in the UDS index. In case the index contains an entry with the same hash, $kvdo$ reads the block from physical device and compare it to the requested block byte-by-byte to ensure they are actually identical.

In case they are, block map is updated in a way that the logical block points to the block on underlying device that has been already written. If the index doesn't contain the computed hash or the block-by-block comparison indetifies a difference in the blocks, $kvdo$ updates the block map and stores the requested block.

Either way, the blocks hash is written to the beginning of the index. This index is held in memory to present quick deduplication advice to the VDO volume.

Logical blocks that are copies and therefore share one physical block are called \emph{shared} blocks.

\subsection{Compression}
Another part of VDO optimisation techniques is compression. By compressig already deduplicated blocks, VDO provides one more step to increase utilisation of underlying device. Compression is also importatnt for saving space in case the incoming data are not well deduplicable.

In VDO, lossless compression, based on a parallelized packaging algorithm is used to handle many compression operations at once. Compressed blocks are stored in a way that allows the most logical blocks to be stored in one physical block.


\subsection{Zero-block elimination}
Zero-blocks are blocks that are composed entirely of zeroes. These blocks are handled differently that normal data blocks.

If the VDO layer detects a zero-block, it will treat it as a discard, thus VDO will not send a write request to the undelying layer. Instead, it will mark the block as free on a physical device (if it was not a shared block) and updates it's block map.

Because of this, if user wants to manually free some space, they can store a file filled with binary zeroes and delete.


\section{Constrains and requirements}
VDO layer is in fact another block device that can aggregate physical storage, partitions etc. On creation of VDO volume, management tool also creates volume for UDS index as well as volume to store actual data.

\subsection{Physical Size}
The VDO volume for physically storing date is divided into continuous regions of physical space of constant size. These regions are called $slabs$ and maybe of size of any power of 2 multiple of $\SI{128}{\mega\byte}$  up to $\SI{32}{\giga\byte}$. After creating VDO volume, the slab size cannont be changed. However, a single VDO volume can contain only up to 8096 slabs, so the configured size of slab at VDO volume creation determines its maximum allowed physical size. Since the maximum slab size is $\SI{32}{\giga\byte}$ and maximum number of slabs is 8096, the maximum volume of physical storage usable by VDO is $\SI{256}{\tera\byte}$. Important thing to notice is that at least one slab will be reserved for VDO metadata and wouldn't be used for storing data. Slab size does not affect VDO performance. As mentioned, at least one $slab$ is reserved for VDO metadata and UDS on-disk index.

VDO module keeps two kinds of metadata which differ in the scale of required space.
\begin{compactenum}
\item type scales with physical size and uses about $\SI{1}{\mega\byte}$ per every $\SI{4}{\giga\byte}$ of managed physical storage and also additional $\SI{1}{\mega\byte}$ per $slab$. 
\item type scales with logical size and uses approximately $\SI{1.25}{\mega\byte}$ for every $\SI{1}{\giga\byte}$ of logical storage, rounded up to the nearest slab.
\end{compactenum}

When trying to examine physical size, thee term Physical size stands for overall size of undelying device. Available physical size stands for the portion of physical size, that can actually hold user data. The part that does not hold user data is used for storing VDO metadata.

\subsection{Logical Size}
The concept of VDO offers a way for users to overprovision the underlying volume. At the time of creation of VDO volume, user can specify its logical size, which can be much larger than the size of physical underlying storage. The user should be able to predict the compressibility of future incoming data and set the logical volume accordingly. At maximum, VDO suports up to 254 times the size of physical volume which amounts to maximum logical size of $\SI{4}{\peta\byte}$.

\subsection{Memory}
The VDO module itself requires $\SI{370}{\mega\byte}$ and additional $\SI{268}{\mega\byte}$ per every $\SI{1}{\tera\byte}$ of used physical storage. Users are therefore expected to compute the needed memory volume and act accordingly.

Another module that consumes memory is the UDS index. However, several mechanisms are in place to ensure the memory consumption does not offset the advantages of VDO usage.

There are two parts to UDS memory usage. First is a compact representation in RAM that contains at most one entry per unique block, that is used for deduplication advice. Second is stored on-disk that keeps track of all blocks presented to UDS. The part stored in RAM tracks only most recent blocks and is called \emph{deduplication window}. Despite it being only index of recent data, most datasets with large levels of possible deduplication also show a high degree of temporal locality, according to developers. This allows for having only a fraction of the UDS index in memory, while still mantaining high levels of deduplication. Were not for this fact, memory requirements for UDS index would be so high that it would out-cost the advantages of VDO usage completely.

For better memory usage, UDS's Sparse Indexing feature was introduced to the uds module. This feature further exploits the temporal locality quality by holding only the most revelant index entries in the memory. Using this feature (which is recommended default for VDO) allows for maintaining up to ten times larger deduplication window while maintaining the same memory requirements.

%\subsection{VDO kernel module}
%The $kvdo$ module provides mentioned techniques within Linux device mapper level. Device mapper serves as a framevork for storage and block device management. The $kvdo$ module presents itself as a block device that can be accessed directly as a raw device or via installation of supported file systems (XFS/EXT4). 

%After receiving read request from an above structure, $kvdo$ maps the requested logical address to the actual physical block and retrieves the data.

%When $kvdo$ receives a write reqest, it updates its block map and acknowledges the request. If the received request is either DISCARD, TRIM or a block of only zeroes, $kvdo$ only acllocates a physical block for the request.


\subsection{CPU}

\section{Internal structures}
Working VDO instance contains structures that handle the incomming events.

\subsection{Slabs}

\subsection{Recovery journal}
Recovery journal provides track of all block changes that has yet to be fully, reliably written to the physical device. It provides performance improvement with bothsynchronous and asynchronous writing policies.

When in synchronous mode, the completion request doesn't wait for the change to be made permanent on the device, it merely waits for the ackowledgement from the journal.

In asynchronous mode, the journal helps providing data loss window by ensuring the user will not lose data if the changes are commited to the journal before the window is expired.

The recovery journal has two parts. One is stored on the physical device and the other in memory to serve as a buffer. When entry is added to the journal, it is processed by the part in memory and is regarded as an active block. An attempt to commit the block to the device is made, however, the device might be locked by another commit thats in progress which makes the commit queue. Every successfull commit will wake others waiting after it is completed.


\subsection{Block map cache}
Block map exists in VDO volume to maintain the mapping of logical blocks to physcial blocks and managed as pages with each page holding 812 entries of logical blocks. The entire block map is kept on disk, since it can be rather large.

Block map cache is a subset of the entire block map that is kept in memory for performance increase. If request to access a block comes to VDO, it will check if that block is in the block map cache. In case it doesn't it needs to read the relevant page and store it in a cache. In case the request was write, it is written in the block map cache and only participates to the on-disk part later.

The block map is a cause of one of the requirements on physical space. It usually consumes about 1.25GB per 1 TB of saved stored physical data. The subset that's kept in memory is much smaller, 128MB by default and is tunable by --vdoBlockMapCacheSize.

The 128MB can cover about 100GB of logical blocks. However if users logical space is larger than 100GB and their workload is so random that it doesn't cached pages, user can observe great preformance hit. In case the block map cache is full, therefore runs out of free pages and a request comes for a page thats not contained in the cache, VDO needs to first discard some page from the cache, write it on disk and just after then load the desired page. It is obvious that this cycle is very time expensive and therefore users are encouraged to increase block map cache size if the precoditions for cache tiering are met.





\subsection{VDO threads}

\subsection{UDS index}
UDS index is a structure designed specifically to indentify duplicate data using hash fingerprints of data blocks. It exists in VDO to provide deduplication advice for effective deduplication. Instance of UDS index is not vital for VDO block handling. In case of losing the index, the VDO still manages blocks and stores and compress data, only without deduplication. Even in the event of index becoming corrupted, there are different mechanisms in place to assure data correctness, so user can discard the index and start bulding a new one.

Also, updating index is costly, so VDO is trying to minimise the updates. That is a reason the index can contain references to blocks that are no longer present in data. Those blocks are called stale blocks.

\section{Tunables}

\subsection{VDO write policies}
VDO can operate in either synchronous or asynchronous mode. By default VDO write policy is set to $auto$ which means the the module decides automatically which write policy to enquire. The main difference is in the approach if the block is written immediately or not. The obvious consequence is that if a system fails while VDO performs asynchronous write, user can lose data.

In synchronous mode, the block is temporarily written to an allocated block and acknowledges the request. After completing the acknowledgement, it attempts to deduplciate the block. In case it's a duplicate, block map is updated in a way that the logical block points to the physical block that's already written and releases the temporal block. If the block is not a duplicate, $kvdo$ updates the block map to make the temporarily allocated physical block permanent.

In asynchronous mode, instead of writing the data immediately, only physical block allocation and acknowledgement of the request is performed. Next, VDO will attempt to deduplicate the block. If the block is a duplicate, the module only updates it's block map and releases the allocated block. If the block turns out to be unique, it is then written to the allocated block and the block map is updated.

\subsection{Block map cache size}

\subsection{VDO threads}

\subsection{Discard size}


%\subsection{VDO in Storage Stack}
%\label{stack}
%Generally it is importatnt for users to realise that some of the storage layers work better when above or under the VDO layer in the storage stack.

%Technology that is recommended to be installed under VDO layer:

%\begin{itemize}
%  \item dm-multipath
%  \item dm-crypt, i.e. layer for data encryption
%  \item mdraid, i.e. software raid
%  \item LVM (as software raid)
%\end{itemize}

%Technology that is recommended to be installed above VDO layer:

%\begin{itemize}
%  \item LVM cache, i.e. possibility to mark part of a block device as a cache to be used by LVM
%  \item LVM Snapshots
%  \item LVM Thin Provisioning
%\end{itemize}

%Unsupported configurations are the ones that break those rules plus a few others:

%\begin{itemize}
%  \item VDO on top of VDO volume
%  \item VDO on top of LVM Snapshots
%  \item VDO on top of LVM Cache
%  \item VDO on top of LVM Thin pool
%  \item VDO on top of a loopback device
%  \item VDO under an encrypted device
%  \item Partitions on VDO volume
%  \item RAIDs on top of VDO volume          
%\end{itemize}

%\section{Administering VDO}
%\subsection{Installation}
%Since VDO is now part of Kernel, it can be installed usign native packaging system. VDO relies on two RPM packages to be installed:
%\begin{itemize}
%    \item vdo
%    \item kmod-kvdo
%\end{itemize}
%After succesfull installation of these two packages, user can create a VDO volume.

%\subsection{Creating VDO volume}
%VDO can be created using VDO manager through command line by executing \texttt{vdo create}.
%The required parameters are:
%\begin{itemize}
%    \item --name=vdoname
%    \item --device=blockdevice
%    \item --vdoLogicalSize=logicalsize
%\end{itemize}

%When specifying block device, it is reccommended to use persistent device name. Otherwise, VDO might fail to start properly in case the name of the device changes.

%While specifying the logical space, user should be aware what kind of data will be written into the VDO block device and set the logical size accordingly. If heavily compressible data are expected, user can specify logical size as large as ten times the physical size. If the data are expected to be less compressible, it is reccomended to lower the ratio accordingly.

%After succesfull VDO creation, the layer is prepared to be used as an ordinary block device. That means, either file system can be created on top of it, or a more complex structure can be installed above. All within the contstrains specified in section~\ref{stack}.

%\subsection{Monitoring VDO}
%VDO works as a thinly provisioned volume. Therefore applications or file systems that use it will only see a logical space that is provided by VDO. To monitor VDO space usage, physical space left or compression ratio and much more, $vdostats$ utility provides neccessary inspection of VDO volume.

\chapter{Benchmarks}
\label{benchmarks}

\section{fs-drift}
Fs-drift is an open-source benchmark developed specifically for testing heavy workload and aging performance. Being implemented in Python, it is very easy for users or contributors to add new features or change the benchmark behavior to their needs. For testing performance of VDO, several new features were added.

%\subsection{New features}
%Fs-drift was used as a main tool in a previous study of file system aging so only new features will be presented in this thesis.
For performance testing of VDO, new features and behavior needed to be implemented to fs-drift to enable more precise control over IOs and testing process.

\subsection{Compressible data generator}
While testing compression and deduplication technology as VDO, data generated for testing need to be specificaly shaped. The benchmark needs to be able to generate the testing data in a way that corresponds to examined cases. A restriction for VDO testing is that the repeating patterns in compressible data cannot be composed entirely of zeroes, because the way VDO deals with zero blocks (see, Zero blocks elimination). For this reason, new parameter was added to fs-drift, that enables the user to produce buffers using $lzdatagenerator$ with specified compressibility.

The parameter works as follows:
\begin{itemize}
    \item -c|--compression-ratio, number that is a desired compress ratio, e.g. 4.0 is 1/4.0, therefore compressibility will be 75\%~(default~0.0)
\end{itemize}

This feature was implemented using alternative IO buffer behavior which works with another open source project called $lzdatagen$.

LZ data generator (i.e. lzdatagen) is a simple but powerfull tool for generating data with desired compressibility. In the simplest use cases, user only specifies the desired size and compress ration and obtains data with the exact qualities.

Usage is:
\begin{itemize}
    \item ./lzdatagen --size SIZE --ratio RATIO
\end{itemize}

If there is a compression ratio set other than 0.0, the buffer is filled using lzdatagen call with the desired compression ratio. Otherwise the buffers are filled the same way as in previous versions of fs-drift.

This simple but powerfull feature now empowers the benchmark user to specify the compressibility of written data and therefore makes the user able to measure performance of any technology that deals with compression feature such as ZFS or VDO.


\subsection{DirectIO}
When researching behavior of random operations in fs-drift, file system cache can often skew considerable amount od measurements.

%This could affect reads, since the blocks could be read from the cache instead of the device.

The system cache considereably affects both writes and reads, and the effect is mostly visible with random operations. In fs-drift, $fsync$ time was taken into the measurement to get the real request completion time. However, the system cache succesfully serialises the requests, so at the time of fsync, they are written sequentially.

To battle this effect, option to use direct IO was added. When passing os.O\_DIRECT flag to the os.open() call, the UNIX based systems require memory alignemnt to the underlying device so all the operations had to be refactored into being aligned.

\subsection{Rawdevice}
While file system can be installed on a VDO layer (and there is an increasing number of users which do), it is important to have an option to test VDO block device also without the file system to get more precise measueremts. File systems can have considerable impact on performance and can skew results in ways that make it hard to clearly observe VDO impact on overall performance.

Rawdevice mode was added, that enables fs-drift to run block-wise on a given device instead of working with files on a file system.

This option is triggered as follows:

The parameter works as follows:
\begin{itemize}
    \item -R|--rawdevice, set path of the device to use it for rawdevice testing (default '')
\end{itemize}


\subsection{Random discard operation}
With thinly provisioned technology being increasingly relevant, it is interesting for researchers to also test performance of a block discards. Discard command could be passed either from the file system or an application to let the underlying device know that the particular block is no longer occupied and can be returned to the block pool.

In Linux, available command for block discard is $blkdiscard$. User can specify the offset and length to be discarded, making it very easy to write an operation type for fs-drift.

If the user of fs-drift wants to test discard speed, he can specify so in the configuration file along with its probability. When the event of discard is triggered, fs-drift works similar as with random writes, but intead of producing buffer and writing, it's using $blkdiscard$ with random offset and specified request size to discard blocks.

\subsection{Random map}
When computing offset for random operations, it might not enough to just generate random number. Sometimes it is benefical to administer IOs only to unused blocks, ensuring no overwriting takes place and all the free blocks will eventually be used.

For this purposes a feature to keep track of unused offests was added to fs-drift. The random map is generated before the test as a shuffled list of possible indices, to save time while the workload is in progress.

The user should be wary of the fact that if the test runs out of the random map, it is recomputed again and will start to overwrite data.

The parameter works as follows:
\begin{itemize}
    \item -r|--randommap, if true, use random map to get random offsets (default False)
\end{itemize}


\subsection{Performance measurement}
In fs-drift, performance is measured by saving a time stamp before invoking the IO operation and storing the time stamp difference after the operation was finished. However, with this type of measurment, it is important to make sure the time stamping is as close to the actual IO operations as possible. 

In previous versions, the time measurement was the same for every IO operation, which made some of the measurement incosistent, e.g. taking into the measurement the time to fill buffer etc.

This problem was removed by moving the time stamps inside the functions, providing more precise control over the timing of events.
%This problem was removed in the latest versions. Changes to the code were made in a way that for every IO operation, the measurements are taken exactly before and after the operation, making them more precise.

Another small feature added by switching to $perf\_counter()$ as a tool to log time instead of $time()$.


\subsection{Data reporting}
In previous versions, several problems in measurement reporting had to be addressed.

One of the problems was the data storage. In older versions, data were stored in-memory which posed two problems when running very long tests:
\begin{itemize}
    \item RAM consumption
    \item Lose of data after benchmark failure
\end{itemize}

The new approach that was installed uses a file predestined on a stable device (i.e. system device). Every time the measurement is made, an entry is appended to the file. This means that even after many days of testing, the memory of a system si not cluttered. On top of that, if the system or benchmark fails, there is still some data to be retreived from the file.

Second problem that caused noisy performance data in the previous versions was the use of response times as units of performance measurement. However, since there could be a large range of file sizes, this unit introduces noise, because larger files will take longer to process in what could seem like a decreased performance.

Therefore, the option to store bandwidth was added as a new feature. Bandwidth is measuerd as a total IO size divided by the time of completion. This way, variability of request size will not be affecting the data points.



\section{FIO}
Flexible Input/Output tester is a workload generator known for its flexibility and large base of users and contributors. It is an exellent benchmark for testing block devices and file systems since it gives users very precise control over the workload.

\subsection{for testing VDO}


\chapter{Testing methodology}
This chapter presents used testing hardware, preparation of testing environment and performance measuring methodology.


\section{Testing environment}
While testing performance, usage of clean testing environment is strongly encouraged. In time of testing, no other applications should run in the environment to ensure low noise levels. Running tests on clean installation of OS is prefered to ensure no performance impacts caused OS aging, memory shortage, etc.

For this thesis, testing was conducted on instances of RHEL-8.1 and RHEL-8.2 to gain access to the most recent features. Tuning options for the systems were set to $throughput-performance$. Other options for the OS are left to be default.

Storage stack for testing purposes is always prepared by executing a seuqence of LVM commands. For the simples tests, one volume group and one logical volume was used to be a block device for VDO layer.

The storage stack can be tested either as a raw deveice, or file system can be installed on top of it when working with files, or relationship between stack and file system needs to be examined.

It is important to create a fresh instance of stack before the test to ensure stable testing conditions. Also, before every test, we $sync$ and $drop caches$. 





\section{Testing hardware}
Testing for this thesis was conducted on multiple machines provided by Red Hat company. I will introduce testing systems which will be used for multitude of tests with or without the VDO layer installed. These machines were chosen by their computing power, provided memory and by useful storage hardware they are equiped with. These machines are stable systems used by Red Hat Kernel Performance team for regular testing.

\label{machines}
\section{Supermicro X11SPL-F}
This machine is used for regular testing of VDO and other complex technologies in Red Hat Kernel Performance team. It's equiped with 4 $\SI{10}{\tera\byte}$ SAS rotational drives and one $\SI{220}{\giga\byte}$ SATA SSD for tests that require LVM Cache. The system is always installed on an additional SSD. This machine is equiped with enough memory to handle large VDO volumes.
\clearpage
\begin{tabular}{|l|l|}
\hline
   \multicolumn{2}{|l|}{Machine\,1} \\ \hline %draven
    Model & Supermicro X11SPL-F\\
    \hline
    Processor & Intel Xeon Silver 4110  \\
    \hline
    Clock speed & $\SI{2.10}{\giga\hertz}$ (8 cores) \\
    \hline
    Memory & $\SI{49152}{\mega\byte}$ \\

\hline
   \multicolumn{2}{|l|}{Testing Hard Drives (4x)} \\ \hline %draven
    Model & WD HGST Ultrastar\\
    \hline
    Capacity & $\SI{1}{\tera\byte}$  \\
    \hline
    Interface & SAS $\SI{12}{\giga\byte}$  \\
    \hline
    Type & Rotational HDD \\
    \hline    
    Logical sector size & $\SI{4096}{\byte}$ \\    
    \hline    
    Physical sector size & $\SI{4096}{\byte}$ \\
    \hline
    
    \multicolumn{2}{|l|}{Testiting SSD (for LVM cache)} \\ \hline %draven
     Model & Micron 5100 MTFD \\
    \hline
     Capacity & $\SI{240}{\giga\byte}$  \\
    \hline
    Interface & SATA $\SI{6}{\giga\byte}$  \\
    \hline
    Type & SSD \\
    \hline    
    Logical sector size & $\SI{512}{\byte}$ \\    
    \hline    
     Physical sector size & $\SI{4096}{\byte}$ \\
    \hline
    
    \multicolumn{2}{|l|}{System disk} \\ \hline %draven
    Model & SuperMicro SSD  \\
    \hline
    Capacity & $\SI{126}{\giga\byte}$  \\
    \hline
    Interface & PCIe Gen3 x4 Lanes  \\
    \hline
    Type & SSD \\
    \hline    
   Logical sector size & $\SI{512}{\byte}$ \\    
    \hline    
    Physical sector size & $\SI{512}{\byte}$ \\
    \hline   
\end{tabular}
\section{Data processing}


\chapter{Performance of VDO}
\label{testing}
\section{Empty VDO}
When preparing VDO volume for testing it is important to notice that a logical space of a fresh instance of VDO is not yet fully allocated. Only after writing data, the volume becomes progressively more allocated. Every new subsequent block of data that is written to VDO has to allocate one to four new blocks. This could affect writing performance of new VDO volume.

To examine this effect, random write workload can be used on a new instance of VDO. Results can be compared to a performance of a fully allocated VDO volume or to a device without VDO.

To obtain fully allocated, but empty VDO volume, it is sufficient to just write one zero byte to every VDO sector. One VDO sector is 812 VDO blocks (default block size in VDO is 4096), so we'll write a zero byte every 812*4096 bytes. 








\section{Half empty VDO}
\section{Block map cache}
As mentioned in chapter 2, block map cache is an internal VDO structure that keeps part of the journal in memory to provide better performance. Its default size is $\SI{128}{\mega\byte}$, which is XXX pages, that covers about $\SI{100}{\giga\byte}$ worth of data. 

This could pose a performance problem in case the logical space is large enough and the workload access the space in a way the block map cache is tired out. If the block map cache fills up, (runs out of free pages), and there is a write request to a non-cached page, the cache is forced to discard one page from its pool to make room for the relevant page. This operation is very costly and this could become a bottleneck for VDO performance.


%If the block map cache runs out of free pages, it is forced to discard one page and load the need one (which is written to) from disk. This operation is very costly and this could become a bottleneck for VDO performance.

To test this effect, we can use random write workload on an instance with sufficient and insufficient cache size. The parameter to tune cache size in VDO is --blockMapCacheSize=BYTES.

%To ensure the workload is truly aggresive, parameter randommap and large file size is used so the test makes lots of requests in a short amount of time and there is no chance that the requested blocks will be in the cache.

To ensure the workload is truly aggresive, large file size and completely random write is used so the test makes lots of requests in a short amount of time and the effect of caching pages could be seen.



\section{VDO threads}
\section{Discard size}
\section{Steady state testing}
\section{Testing on different architectures}
\section{Journal performance}
\section{Testing with distributed file system}

\chapter{Conclusion}
\label{conclusion}












%\chapter{Tested configurations}
%\label{storageConf}
%In testing of specific storage stack layer, it is important to have performance assesment of a stack without that layer installed. These configurations are called baseline configurations. In this thesis, baseline configurations will used striped LVM to join hard drives as well as LVM cache to simulate real-life usage.

%After the baseline testing is finished, tests with VDO layer installed will be run. For non-cached version, only simple VDO layer will be installed with file system on top of it. In the cached version, VDO layer will be installed under the LVM-cache.


%\section{Baseline configurations}
%Baseline configurations will be storage stacks without the VDO layer installed. Their purpose is to provide a stable testing enviroment to determine performance of undelying hardware and performance of other layers in storage stack.

%\subsection{Striped LV}
%This baseline configuration will consist of simple stack built with LVM on top of two rotational hard drives. The striping of 2 will be used to balance the load on both devices equally. The commands %to create this simple stack is shown in example~\ref{ex:simpleLV}

%\lstset{language=bash, 
%numbers=none, 
%frame=single, 
%commentstyle=\color{dkgreen}, 
%basicstyle={\scriptsize\ttfamily}, 
%keywordstyle=\color{blue}, 
%identifierstyle=\color{blue}, 
%stringstyle=\color{red},
%captionpos=t,
%showstringspaces=false,
%breaklines=true,
%breakatwhitespace=false,
%tabsize=3,
%caption={sdad},
%}


%\begin{lstlisting}[language=bash, label={ex:simpleLV}, caption={Creating striped logical volume}][frame=single]
%    $ pvcreate /dev/sd[bc] -y
%    $ vgcreate vg /dev/sd[bc] -y
%    $ lvcreate -n testLV -i2 -l100%vg vg /dev/sd[bc] -y
%    $ mkfs.xfs /dev/mapper/vg-testLV        
%\end{lstlisting}

%\subsection{Cached LV}
%This baseline configuration will consist of a striped LV over two rotational hard drives supported with an LVM Cache hosted on a separate solid state device. These devices must belong to the same Volume Group. Creating LVM Cache consist of several steps. First a small part of the SSD needs to be reserved for LVM Cache metadata. This can be done by creating small LV on the device and converting that LV to cache-pool type. Next, we need to mark the portion of SSD we want to use for caching by creating another LV. We'll create another LV to join the two hard drives with striping of 2. At the end, we convert the storage LV to a cached volume. The commands to create this cached volume is shown in example~\ref{ex:cachedLV}

%\begin{lstlisting}[language=bash, label={ex:cachedLV}, caption={Creating cached logical volume}][frame=single]
 %   $ pvcreate /dev/sd[bce] -y
  %  $ vgcreate vg /dev/sd[bce] -y
   % $ lvcreate -n cacheMeta -L 44M vg /dev/sde -y
    %$ lvcreate -n cache -L 200G vg /dev/sde -y
%    $ lvcreate -n testLV -i2 -l100%vg vg /dev/sd[bc]  -y
 %   $ lvconvert --type cache-pool --poolmetadata vg/cacheMeta vg/cache -y
  %  $ lvconvert --type cache --cachepool vg/cache vg/testLV -y
    %$ mkfs.xfs /dev/mapper/vg-testLV        
%\end{lstlisting}


%\section{Configurations with VDO}
%After determining the performance without VDO layer installed, we can continue with actual performance testing of VDO. Important thing to notice is that when creating a file system on top of VDO volume, the file system will attempt to discard the whole volume to ensure a clean begining state. While testing this isn't really an important feature and with volumes of tens of tera bytes, it would take quite a long time to discard the whole VDO volume. Therefore mkfs options (-K for XFS) can be used to prevent it.

%\subsection{VDO over striped LV}
%This simple configuration will consist of VDO layer on top of two rotational hard drives joined by LV. The striping of 2 will be used to balance the load on both devices equally. The commands to create this stack is shown in example~\ref{ex:VDOLVC}

%\begin{lstlisting}[language=bash, label={ex:VDOLV}, caption={Creating VDO over striped LV}][frame=single]
%    $ pvcreate /dev/sd[bc] -y
 %   $ vgcreate vg /dev/sd[bc] -y
   % $ lvcreate -n testLV -i2 -l100%vg vg /dev/sd[bc] -y
  %  $ vdo create --name=testVDO --device=/dev/mapper/vg-testLV --vdoLogicalSize=60T  --vdoSlabSize=8g --force
    %$ mkfs.xfs -K /dev/mapper/vg-testLV      
%\end{lstlisting}

%\subsection{VDO under LVM Cache}
%This configuration will consist of a VDO layer built on top of two rotational hard drives joined by LV. The striping of 2 will be used to balance the load on both devices equally. Sometimes the LVM can have a problem with adding devices with different block size to a single volume group. This can be avoided by allowign mixed block sizes in lvm configuration file. Also, when more complexity needs to be build above the VDO layer, it is important to create a physical volume on a VDO block device. The commands to create this stack is shown in example~\ref{ex:VDOLVCache}

%\begin{lstlisting}[language=bash, label={ex:VDOLVCache}, caption={Creating cached VDO volume}][frame=single]
%    $ pvcreate /dev/sd[bce] -y
%    $ vgcreate vg /dev/sd[bc] -y
%    $ lvcreate -n testLV -i2 -l100%vg vg /dev/sd[bc]  -y    
 %   $ vdo create --name=testVDO --device=/dev/mapper/vg-testLV --vdoLogicalSize=60T --vdoSlabSize=8g --force    
  %  $ sed -i '/devices {/a allow_mixed_block_sizes = 1' /etc/lvm/lvm.conf
   % $ pvcreate /dev/mapper/testVDO -y
   % $ vgcreate VG2 /dev/mapper/testVDO /dev/sde -y -ff
   % $ lvcreate -n cacheMeta -L 44M VG2 /dev/sde -y
%    $ lvcreate -n cache -L 200G VG2 /dev/sde
 %   $ lvcreate -n testLV2 -l100%vg VG2 /dev/mapper/testVDO
  %  $ lvconvert --type cache-pool --poolmetadata VG2/cacheMeta VG2/cache -y
   % $ lvconvert --type cache --cachepool VG2/cache VG2/testLV2 -y
    %$ mkfs.xfs -K /dev/mapper/VG2-testLV2
%\end{lstlisting}


%\chapter{Conducted tests}
%For thorough testing of VDO, several types of tests needs to be executed. First section will aim to show differences in performance between loading VDO with heavily compressible data and non-compressible data. Next section will try to show differences in performance of VDO layer when under single threaded load and heavy multithreaded load. These tests will be held on all storage configurations from Chapter~\ref{storageConf}.

%\section{Different compressibility}
%The VDO layer can be used in multiple ways, when users want to save physical space. However, the level of compressibility of user data can vary. The aim of this test is to determine, whether compressibility of user data have impact on VDO performance or to what extent.

%\section{Single vs. Multithread}
%Since VDO usage is meant for simple users as well as for complex cases, it is important to know the differences between single-user load and heavy multithreaded traffic.





































\end{document}
